{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indirect-amsterdam",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T21:36:53.137816Z",
     "start_time": "2021-03-14T21:36:53.131991Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve, classification_report\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neither-minister",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T19:43:03.507376Z",
     "start_time": "2021-03-14T19:43:03.293559Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load Dataframes of the different component\n",
    "rooth_path = '../data/'\n",
    "generator_train = pd.read_csv(rooth_path + 'generator_train.csv')\n",
    "gen_bear_train = pd.read_csv(rooth_path + 'gen_bear_train.csv')\n",
    "transformer_train = pd.read_csv(rooth_path + 'transformer_train.csv')\n",
    "hydraulic_train = pd.read_csv(rooth_path + 'hydraulic_train.csv')\n",
    "gearbox_train = pd.read_csv(rooth_path + 'gearbox_train.csv')\n",
    "generator_test = pd.read_csv(rooth_path + 'generator_test.csv')\n",
    "gen_bear_test = pd.read_csv(rooth_path + 'gen_bear_test.csv')\n",
    "transformer_test = pd.read_csv(rooth_path + 'transformer_test.csv')\n",
    "hydraulic_test = pd.read_csv(rooth_path + 'hydraulic_test.csv')\n",
    "gearbox_test = pd.read_csv(rooth_path + 'gearbox_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-dollar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T19:40:41.747585Z",
     "start_time": "2021-03-14T19:40:41.722549Z"
    }
   },
   "source": [
    "### We are ready to build our base-model for each component. \n",
    "\n",
    "- We will need to scale the features, within each turbine\n",
    "- Since we have a very unbalanced dataset we will use SMOTE for balancing the training data set\n",
    "- Reach a base line model\n",
    "- Try other algorithms and fine tune\n",
    "- Probably we will need to change the threshold to reduce the FP\n",
    "- Calculate the different costs of our best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "swiss-rendering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T21:34:42.225381Z",
     "start_time": "2021-03-14T21:34:42.205792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale within each turbine\n",
    "def scale (df_train, df_test, scaler='StandardScaler'):\n",
    "    \n",
    "    '''Scale within each given turbine'''\n",
    "    \n",
    "    # Scale for turbine T01 first\n",
    "    X_train1 = df_train.loc[df_train['Turbine_ID']=='T01']\n",
    "    X_test1 = df_test.loc[df_test['Turbine_ID']=='T01']\n",
    "\n",
    "    X_train1 = X_train1.drop(columns=['Turbine_ID', 'Date', 'TTF', 'Failure'])\n",
    "    X_test1 = X_test1.drop(columns=['Turbine_ID', 'Date', 'TTF', 'Failure'])\n",
    "    \n",
    "    if scaler == 'MinMaxScaler':\n",
    "        sc = MinMaxScaler()\n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1) \n",
    "    else:\n",
    "        sc = StandardScaler()\n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1) \n",
    "    \n",
    "    # Scale on other turbines\n",
    "    turbines = ['T06', 'T07', 'T09', 'T11']\n",
    "    for turbine in turbines:\n",
    "        X_train_ = df_train.loc[df_train['Turbine_ID']==turbine]\n",
    "        X_test_ = df_test.loc[df_test['Turbine_ID']==turbine]\n",
    "\n",
    "        X_train_ = X_train_.drop(columns=['Turbine_ID', 'Date', 'TTF', 'Failure'])\n",
    "        X_test_ = X_test_.drop(columns=['Turbine_ID', 'Date', 'TTF', 'Failure'])\n",
    "\n",
    "        if scaler == 'MinMaxScaler':\n",
    "            sc = MinMaxScaler()\n",
    "            X_train_ = sc.fit_transform(X_train_)\n",
    "            X_test_ = sc.transform(X_test_)\n",
    "        else:\n",
    "            sc = StandardScaler()\n",
    "            X_train_ = sc.fit_transform(X_train_)\n",
    "            X_test_ = sc.transform(X_test_)\n",
    "        \n",
    "        # Concatenate\n",
    "        X_train1 = np.concatenate((X_train1, X_train_))\n",
    "        X_test1 = np.concatenate((X_test1, X_test_))\n",
    "\n",
    "    return X_train1, X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug():\n",
    "    \n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
